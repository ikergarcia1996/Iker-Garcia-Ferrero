---
title: Iker GarcÃ­a-Ferrero
button_title: Blog
layout: default
filename: blog
--- 

# Blog Posts

---

<h2><img src="https://github.com/hitz-zentroa/GoLLIE/blob/main/assets/GoLLIE.png?raw=true" width="25"> GoLLIE: Guideline-following Large Language Model for Information Extraction</h2>
Oscar Sainz, <ins>Iker GarcÃ­a-Ferrero</ins>, Rodrigo Agerri, Oier Lopez de Lacalle, German Rigau and Eneko Agirre

**TL;DR**: We present GoLLIE, a Large Language Model trained to follow annotation guidelines. GoLLIE outperforms previous approaches on zero-shot Information Extraction and allows the user to perform inferences with annotation schemas defined on the fly. Different from previous approaches, GoLLIE is able to follow detailed definitions and does not only rely on the knowledge already encoded in the LLM.

ðŸ“’[Blog Post](https://hitz-zentroa.github.io/GoLLIE/) 

---


## Did ChatGPT cheat on your test? (2023)
Oscar Sainz, Jon Ander Campos, <ins>Iker GarcÃ­a-Ferrero</ins>, Julen Etxaniz, Eneko Agirre  

**TL;DR**: Large Language Models have seen trillions of tokens. However, who knows what is inside? Recent works have evaluated those models in many different tasks, but, did they make sure the model had not already seen the training or even the evaluation datasets? In this blog post, we show that some popular benchmark datasets are already memorized by ChatGPT and that one can prompt ChatGPT to regenerate them.

ðŸ“’[Blog Post](https://hitz-zentroa.github.io/lm-contamination/blog/) 

---
