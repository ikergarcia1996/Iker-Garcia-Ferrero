---
title: Iker GarcÃ­a-Ferrero
button_title: Blog
layout: default
filename: blog
--- 

# Blog Posts

---

## Did ChatGPT cheat on your test? (2023)
Oscar Sainz, Jon Ander Campos, <ins>Iker GarcÃ­a-Ferrero</ins>, Julen Etxaniz, Eneko Agirre  

**TL;DR**: Large Language Models have seen trillions of tokens. However, who knows what is inside? Recent works have evaluated those models in many different tasks, but, did they make sure the model had not already seen the training or even the evaluation datasets? In this blog post, we show that some popular benchmark datasets are already memorized by ChatGPT and that one can prompt ChatGPT to regenerate them.

ðŸ“’[Blog Post](https://hitz-zentroa.github.io/lm-contamination/blog/) 

---
